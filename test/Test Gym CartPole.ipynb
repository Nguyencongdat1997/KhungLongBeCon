{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"colab":{"name":"Test Gym CartPole.ipynb","provenance":[],"collapsed_sections":["70e322b3","1cc0a6f0"],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"cc011312"},"source":["# Installization & Import"],"id":"cc011312"},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":323},"id":"b81d7a86","executionInfo":{"status":"ok","timestamp":1623132543153,"user_tz":-420,"elapsed":32446,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}},"outputId":"ee1e74e0-1936-48ca-a381-88f080fe83d3"},"source":["!pip install tensorflow==2.3.1 gym keras-rl2 gym[atari]"],"id":"b81d7a86","execution_count":1,"outputs":[{"output_type":"stream","text":["  Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Found existing installation: tensorflow-estimator 2.5.0\n","    Uninstalling tensorflow-estimator-2.5.0:\n","      Successfully uninstalled tensorflow-estimator-2.5.0\n","  Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Found existing installation: tensorflow 2.5.0\n","    Uninstalling tensorflow-2.5.0:\n","      Successfully uninstalled tensorflow-2.5.0\n","Successfully installed gast-0.3.3 h5py-2.10.0 keras-rl2-1.0.5 numpy-1.18.5 tensorflow-2.3.1 tensorflow-estimator-2.3.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"3d4450bd","executionInfo":{"status":"ok","timestamp":1623132864249,"user_tz":-420,"elapsed":336,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}}},"source":["import gym \n","import random\n","import time\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras \n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n","from tensorflow.keras.optimizers import Adam"],"id":"3d4450bd","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e866cf0b"},"source":["# Environment"],"id":"e866cf0b"},{"cell_type":"code","metadata":{"id":"13bdfec0","executionInfo":{"status":"ok","timestamp":1623132545207,"user_tz":-420,"elapsed":10,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}}},"source":["env = gym.make('CartPole-v0')\n","observations = env.observation_space.shape[0]\n","actions = env.action_space.n\n","action_space = [x for x in range(actions)]"],"id":"13bdfec0","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06c25dea","executionInfo":{"status":"ok","timestamp":1623136107704,"user_tz":-420,"elapsed":322,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}},"outputId":"7547cbea-120d-4a42-aa6e-11307f8b46ad"},"source":["print(actions)\n","sample_action = env.action_space.sample()\n","print(sample_action)\n","print(observations)\n","state = env.reset()\n","print(state)\n","state, reward, done, info = env.step(sample_action)\n","print(state, reward, done, info)"],"id":"06c25dea","execution_count":27,"outputs":[{"output_type":"stream","text":["2\n","0\n","4\n","[-0.03128079  0.01225896  0.04015555 -0.02856366]\n","[-0.03103561 -0.18341516  0.03958428  0.27651347] 1.0 False {}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"ec66d65b","executionInfo":{"status":"ok","timestamp":1623132569162,"user_tz":-420,"elapsed":336,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}},"outputId":"fd04fa8e-986d-4c9c-b92e-034f75e4b204"},"source":["episodes = 5\n","for episode in range(1, episodes+1):\n","    state = env.reset()\n","    done = False\n","    score = 0 \n","    \n","    while not done:\n","        # env.render()\n","        action = random.choice(action_space)\n","        n_state, reward, done, info = env.step(action)\n","        score += reward\n","    print('Episode:{} Score:{}'.format(episode, score))\n","env.close()"],"id":"ec66d65b","execution_count":6,"outputs":[{"output_type":"stream","text":["Episode:1 Score:20.0\n","Episode:2 Score:16.0\n","Episode:3 Score:15.0\n","Episode:4 Score:12.0\n","Episode:5 Score:43.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8e3da308"},"source":["# KerasRL's DQN"],"id":"8e3da308"},{"cell_type":"markdown","metadata":{"id":"70e322b3"},"source":["## Import"],"id":"70e322b3"},{"cell_type":"code","metadata":{"id":"c79a434d"},"source":["from rl.agents import DQNAgent\n","from rl.memory import SequentialMemory\n","from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy, BoltzmannQPolicy"],"id":"c79a434d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1cc0a6f0"},"source":["## Model"],"id":"1cc0a6f0"},{"cell_type":"code","metadata":{"id":"efdd7f71"},"source":["def build_model(observations, actions):\n","    model = Sequential()\n","    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n","    model.add(Dense(24, activation='tanh'))\n","    model.add(Dense(48, activation='tanh'))\n","    model.add(Dense(actions, activation='linear'))\n","    return model"],"id":"efdd7f71","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f64bd546"},"source":["model = build_model(observations, actions)"],"id":"f64bd546","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f81613b7","outputId":"38ef9b38-2ab3-4a97-e72c-ef2c032690fe"},"source":["model.summary()"],"id":"f81613b7","execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten (Flatten)            (None, 4)                 0         \n","_________________________________________________________________\n","dense (Dense)                (None, 24)                120       \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 48)                1200      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 2)                 98        \n","=================================================================\n","Total params: 1,418\n","Trainable params: 1,418\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ac65c994"},"source":["## DQN"],"id":"ac65c994"},{"cell_type":"code","metadata":{"id":"f87bfcd4"},"source":["def build_agent(model, actions):\n","    memory = SequentialMemory(limit=50000, window_length=1)\n","    policy = BoltzmannQPolicy()\n","    dqn = DQNAgent(model=model, nb_actions=actions, memory=memory, nb_steps_warmup=2000,\n","                   target_model_update=1e-2, policy=policy)\n","    dqn.compile(Adam(lr=0.01, decay=0.01), metrics=['mse'])\n","    return dqn"],"id":"f87bfcd4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"666c9042"},"source":["dqn = build_agent(model, actions)"],"id":"666c9042","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"35a41482","outputId":"f23aa492-3b7f-4418-9865-124dcb5bcf1e"},"source":["dqn.fit(env, nb_steps=10000, visualize=False, verbose=1)"],"id":"35a41482","execution_count":null,"outputs":[{"output_type":"stream","text":["Training for 10000 steps ...\n","Interval 1 (0 steps performed)\n","10000/10000 [==============================] - 76s 8ms/step - reward: 1.0000\n","done, took 76.169 seconds\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x1b85b1d8388>"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"cac100f0"},"source":["dqn.save_weights('./trained_models/CartPole/KeraRL/model_10000')"],"id":"cac100f0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d6a199df"},"source":["## Test"],"id":"d6a199df"},{"cell_type":"code","metadata":{"id":"b57cbef6"},"source":["dqn.load_weights('./trained_models/CartPole/KeraRL/model_10000')"],"id":"b57cbef6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1c125590","outputId":"47a927f2-8f61-42a0-e0f4-6d1628f093a8"},"source":["scores = dqn.test(env, nb_episodes=5, visualize=True)\n","print(np.mean(scores.history['episode_reward']))"],"id":"1c125590","execution_count":null,"outputs":[{"output_type":"stream","text":["Testing for 5 episodes ...\n","Episode 1: reward: 200.000, steps: 200\n","Episode 2: reward: 200.000, steps: 200\n","Episode 3: reward: 200.000, steps: 200\n","Episode 4: reward: 200.000, steps: 200\n","Episode 5: reward: 200.000, steps: 200\n","200.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5d005524","outputId":"9c607654-1211-413d-c177-302fa87f6279"},"source":["episodes = 5\n","for episode in range(episodes):\n","    state = env.reset()\n","    score = 0\n","    while True:\n","        action = dqn.forward(state)\n","        state, reward, done, info = env.step(action)\n","        env.render()\n","        score+=reward\n","        if done:\n","            break\n","    print('Episode: {} score: {}'.format(episode, score))"],"id":"5d005524","execution_count":null,"outputs":[{"output_type":"stream","text":["Episode: 0 score: 200.0\n","Episode: 1 score: 200.0\n","Episode: 2 score: 200.0\n","Episode: 3 score: 200.0\n","Episode: 4 score: 200.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"99c18964"},"source":["# Stable baseline"],"id":"99c18964"},{"cell_type":"markdown","metadata":{"id":"0410385b"},"source":["## Import"],"id":"0410385b"},{"cell_type":"code","metadata":{"id":"a3dd36ce"},"source":["from stable_baselines3.common.cmd_util import make_atari_env\n","from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n","from stable_baselines3 import PPO, DQN, A2C\n","from stable_baselines3.common.callbacks import BaseCallback\n","from stable_baselines3.common.evaluation import evaluate_policy\n","import numpy as np \n","import os## Import"],"id":"a3dd36ce","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ae4e04aa"},"source":["## Callback"],"id":"ae4e04aa"},{"cell_type":"code","metadata":{"id":"e6b3ce28"},"source":["class SavingBestTrainingRewardCallback(BaseCallback):\n","    def __init__(self, check_freq:int, save_path: str, verbose=1):\n","        super(SavingBestTrainingRewardCallback, self).__init__(verbose)\n","        self.check_freq = check_freq\n","        self.save_path = save_path\n","    def _init_callback(self):\n","        if self.save_path:\n","            os.makedirs(self.save_path, exist_ok=True)\n","    def _on_step(self):\n","        if self.n_calls % self.check_freq == 0:\n","            model_path = os.path.join(self.save_path, 'model_{}'.format(self.n_calls))\n","            self.model.save(model_path)\n","        return True"],"id":"e6b3ce28","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a582de2e"},"source":["CHECKPOINT_DIR = './trained_models/CartPole/StableBaselines/'\n","LOG_DIR = './logs/CartPole/StableBaselines/'\n","callback = SavingBestTrainingRewardCallback(check_freq=1000, save_path=CHECKPOINT_DIR)"],"id":"a582de2e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"716a6148"},"source":["## Train"],"id":"716a6148"},{"cell_type":"code","metadata":{"id":"848ce634"},"source":["agent = A2C('MlpPolicy', env, verbose=0, tensorboard_log=LOG_DIR)\n","# agent = DQN('MlpPolicy', env, verbose=0, tensorboard_log=LOG_DIR)\n","#agent = ACER('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR)\n","#agent = PPO2('CnnPolicy', env, minibaches=2, verbose=1, tensorboard_log=LOG_DIR)\n","#agent = DQN('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR)"],"id":"848ce634","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"66e2d10f"},"source":["# trained_agent = A2C.load('./train/model_10000', env=env, tensorboard_log=LOG_DIR)"],"id":"66e2d10f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"411d4c48","outputId":"f5942313-0c22-4a7c-9231-19f9d92c5874"},"source":["agent.learn(total_timesteps= 20000, callback= callback)"],"id":"411d4c48","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<stable_baselines3.a2c.a2c.A2C at 0x1bb5fae4908>"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"8119e0ef"},"source":["## Test"],"id":"8119e0ef"},{"cell_type":"code","metadata":{"id":"21ec0994"},"source":["agent = A2C.load(CHECKPOINT_DIR + '/model_20000', env=env)"],"id":"21ec0994","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"db76df9c","outputId":"d20ec380-be0b-4910-814a-ab0b4020db48"},"source":["evaluate_policy(agent, env, n_eval_episodes=10, render=True)"],"id":"db76df9c","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(200.0, 0.0)"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"e1bfa415","outputId":"4e44379e-58e8-42dd-9198-d596a01d9a53"},"source":["episodes = 5\n","for episode in range(episodes):\n","    state = env.reset()\n","    score = 0\n","    while True:\n","        action, states = agent.predict(obs)\n","        obs, reward, done, info = env.step(action)\n","        env.render()\n","        score+=reward\n","        if done:\n","            break\n","    print('Episode: {} score: {}'.format(episode, score))"],"id":"e1bfa415","execution_count":null,"outputs":[{"output_type":"stream","text":["Episode: 0 score: 197.0\n","Episode: 1 score: 37.0\n","Episode: 2 score: 191.0\n","Episode: 3 score: 118.0\n","Episode: 4 score: 199.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"792526cf"},"source":["# From scratch - Dueling Deep Q - Keras"],"id":"792526cf"},{"cell_type":"markdown","metadata":{"id":"CM8ybh7UQ5kI"},"source":["## Replay Buffer"],"id":"CM8ybh7UQ5kI"},{"cell_type":"code","metadata":{"id":"kfRYechIQ7zO","executionInfo":{"status":"ok","timestamp":1623136388836,"user_tz":-420,"elapsed":317,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}}},"source":["class ReplayBuffer():\n","  def __init__(self, max_size, input_shape):\n","    self.mem_size = max_size\n","    self.mem_counter = 0\n","    \n","    self.states = np.zeros((self.mem_size, *input_shape), dtype=np.float64)\n","    self.next_states = np.zeros((self.mem_size, *input_shape), dtype=np.float64)\n","    self.rewards = np.zeros(self.mem_size, dtype=np.float64)\n","    self.actions = np.zeros(self.mem_size, dtype=np.int32)\n","    self.done = np.zeros(self.mem_size, dtype=np.bool)\n","\n","  def store_step(self, state, action, reward, next_state, done):\n","    index = self.mem_counter % self.mem_size\n","    self.states[index] = state\n","    self.next_states[index] = next_state\n","    self.actions[index] = action\n","    self.rewards[index] = reward\n","    self.done[index] = done\n","    self.mem_counter += 1\n","\n","  def sample_buffer(self, batch_size):\n","    max_mem = min(self.mem_counter, self.mem_size)\n","    batch = np.random.choice(max_mem, batch_size, replace=False)\n","\n","    states = self.states[batch]\n","    next_states = self.next_states[batch]\n","    rewards = self.rewards[batch]\n","    actions = self.actions[batch]\n","    done = self.done[batch]\n","\n","    return states, actions, rewards, next_states, done\n","    "],"id":"kfRYechIQ7zO","execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Xn9jbTsPjI2"},"source":["## Q Network"],"id":"6Xn9jbTsPjI2"},{"cell_type":"code","metadata":{"id":"O09zF-WHPhFv","executionInfo":{"status":"ok","timestamp":1623136120292,"user_tz":-420,"elapsed":478,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}}},"source":["class DuelingDeepQNetwork(keras.Model):\n","  def __init__(self, n_actions):\n","    super(DuelingDeepQNetwork, self).__init__()\n","\n","    fc1_dims = 128\n","    fc2_dims = 128\n","    self.dense1 = keras.layers.Dense(fc1_dims, activation='relu')\n","    self.dense2 = keras.layers.Dense(fc2_dims, activation='relu')\n","    self.V = keras.layers.Dense(1, activation=None)\n","    self.A = keras.layers.Dense(n_actions, activation=None)\n","\n","  def call(self, state):\n","    x = self.dense1(state)\n","    x = self.dense2(x)\n","    V = self.V(x)\n","    A = self.A(x)\n","\n","    Q = (V + (A - tf.math.reduce_mean(A, axis=1, keepdims=True)))\n","    return Q\n","\n","  def advantage(self, state):\n","    x = self.dense1(state)\n","    x = self.dense2(x)    \n","    A = self.A(x)\n","    return A  \n"],"id":"O09zF-WHPhFv","execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EO4dmDllS5ng"},"source":["## Agent"],"id":"EO4dmDllS5ng"},{"cell_type":"code","metadata":{"id":"ub0jKpvgS7U2","executionInfo":{"status":"ok","timestamp":1623139203712,"user_tz":-420,"elapsed":315,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}}},"source":["class Agent():\n","  def __init__(self, lr, gamma, n_actions, epsilon, batch_size, input_dims, epsilon_dec=1e-3, epsilon_end=0.01, mem_size=1000000, replace=100):\n","    self.action_space = [i for i in range(actions)]\n","    self.gamma =gamma\n","    self.epsilon = epsilon\n","    self.epsilon_dec = epsilon_dec\n","    self.epsilon_end = epsilon_end\n","    self.replace = replace\n","    self.batch_size = batch_size\n","\n","    self.learned_step_counter = 0\n","    self.memory = ReplayBuffer(mem_size, input_dims)\n","    self.q_active =  DuelingDeepQNetwork(n_actions)\n","    self.q_frozen =  DuelingDeepQNetwork(n_actions)\n","\n","    self.q_active.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n","    self.q_frozen.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n","\n","  def store_step(self, state, action, reward, next_state, done):\n","    self.memory.store_step(state, action, reward, next_state, done)\n","  \n","  def choose_action(self, observation):\n","    if np.random.random() < self.epsilon:\n","      action = np.random.choice(self.action_space)\n","    else:\n","      state = np.array([observation])\n","      actions = self.q_active.advantage(state)\n","      action = tf.math.argmax(actions, axis=1).numpy()[0]\n","    return action\n","\n","  def learn(self):\n","    if self.memory.mem_counter < self.batch_size:\n","      return\n","    \n","    if self.learned_step_counter % self.replace == 0:\n","      self.q_frozen.set_weights(self.q_active.get_weights())\n","\n","    # get data\n","    states, actions, rewards, next_states, dones = self.memory.sample_buffer(self.batch_size)\n","    q_pred = self.q_active(states)\n","    q_next = self.q_frozen(next_states)\n","    q_target = q_pred.numpy()\n","    max_next_actions = tf.math.argmax(self.q_active(next_states), axis=1)\n","    for i, terminated in enumerate(dones):\n","      q_target[i, actions[i]] = rewards[i] + self.gamma*q_next[i, max_next_actions[i]]*(1-int(dones[i]))\n","\n","    # train\n","    self.q_active.train_on_batch(states, q_target)\n","\n","    self.epsilon = max(self.epsilon - self.epsilon_dec, self.epsilon_end)\n","    self.learned_step_counter += 1\n","\n","  def train(self, env, n_games):\n","    scores = []\n","    eps_history = []\n","    steps = 0\n","    for i in range(n_games):\n","      done = False\n","      score = 0\n","      observation = env.reset()\n","      while not done:\n","        steps += 1\n","        action = self.choose_action(observation)\n","        next_observation, reward, done, info = env.step(action)\n","        score += reward\n","        self.store_step(observation, action, reward, next_observation, done)\n","        observation = next_observation\n","        self.learn()\n","      eps_history.append(self.epsilon)\n","      scores.append(score)\n","      avg_score = np.mean(scores[-10:])\n","      print('Episode', i, '- trained steps', steps, '- score %.1f'%score, '- avg_score %.1f ' % avg_score)\n","\n","  def save_model(self, train_dir):\n","    file_name = train_dir + '/d3qn_' + str(self.learned_step_counter) + '/model'\n","    #self.q_active.save_weights(file_name)\n","    self.q_active.save_weights(file_name, save_format='tf')\n","\n","  def load_model(self, train_dir, learned_steps = 100):\n","    file_name = train_dir + '/d3qn_' + str(learned_steps) + '/model' \n","    self.q_active.load_weights(file_name)\n","    self.q_frozen.set_weights(self.q_active.get_weights())\n"],"id":"ub0jKpvgS7U2","execution_count":129,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CcUQlfdjXUIG"},"source":["## Train"],"id":"CcUQlfdjXUIG"},{"cell_type":"code","metadata":{"id":"OxkII7xYXUfk","executionInfo":{"status":"ok","timestamp":1623139616602,"user_tz":-420,"elapsed":353,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}}},"source":["d3qn = Agent(lr=0.005, gamma=0.99, n_actions=env.action_space.n, epsilon=1.0, batch_size=64, input_dims=env.observation_space.shape)"],"id":"OxkII7xYXUfk","execution_count":145,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7NUIsVPCZ3nW","executionInfo":{"status":"ok","timestamp":1623140378431,"user_tz":-420,"elapsed":523050,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}},"outputId":"60261065-9012-4be4-fb65-6f081ba33aa4"},"source":["n_games = 100\n","d3qn.train(env, n_games)"],"id":"7NUIsVPCZ3nW","execution_count":154,"outputs":[{"output_type":"stream","text":["Episode 0 - trained steps 91 - score 91.0 - avg_score 91.0 \n","Episode 1 - trained steps 177 - score 86.0 - avg_score 88.5 \n","Episode 2 - trained steps 272 - score 95.0 - avg_score 90.7 \n","Episode 3 - trained steps 367 - score 95.0 - avg_score 91.8 \n","Episode 4 - trained steps 458 - score 91.0 - avg_score 91.6 \n","Episode 5 - trained steps 562 - score 104.0 - avg_score 93.7 \n","Episode 6 - trained steps 666 - score 104.0 - avg_score 95.1 \n","Episode 7 - trained steps 774 - score 108.0 - avg_score 96.8 \n","Episode 8 - trained steps 881 - score 107.0 - avg_score 97.9 \n","Episode 9 - trained steps 985 - score 104.0 - avg_score 98.5 \n","Episode 10 - trained steps 1088 - score 103.0 - avg_score 99.7 \n","Episode 11 - trained steps 1207 - score 119.0 - avg_score 103.0 \n","Episode 12 - trained steps 1319 - score 112.0 - avg_score 104.7 \n","Episode 13 - trained steps 1402 - score 83.0 - avg_score 103.5 \n","Episode 14 - trained steps 1508 - score 106.0 - avg_score 105.0 \n","Episode 15 - trained steps 1610 - score 102.0 - avg_score 104.8 \n","Episode 16 - trained steps 1737 - score 127.0 - avg_score 107.1 \n","Episode 17 - trained steps 1862 - score 125.0 - avg_score 108.8 \n","Episode 18 - trained steps 1992 - score 130.0 - avg_score 111.1 \n","Episode 19 - trained steps 2111 - score 119.0 - avg_score 112.6 \n","Episode 20 - trained steps 2120 - score 9.0 - avg_score 103.2 \n","Episode 21 - trained steps 2254 - score 134.0 - avg_score 104.7 \n","Episode 22 - trained steps 2385 - score 131.0 - avg_score 106.6 \n","Episode 23 - trained steps 2525 - score 140.0 - avg_score 112.3 \n","Episode 24 - trained steps 2664 - score 139.0 - avg_score 115.6 \n","Episode 25 - trained steps 2789 - score 125.0 - avg_score 117.9 \n","Episode 26 - trained steps 2917 - score 128.0 - avg_score 118.0 \n","Episode 27 - trained steps 3057 - score 140.0 - avg_score 119.5 \n","Episode 28 - trained steps 3257 - score 200.0 - avg_score 126.5 \n","Episode 29 - trained steps 3317 - score 60.0 - avg_score 120.6 \n","Episode 30 - trained steps 3517 - score 200.0 - avg_score 139.7 \n","Episode 31 - trained steps 3672 - score 155.0 - avg_score 141.8 \n","Episode 32 - trained steps 3798 - score 126.0 - avg_score 141.3 \n","Episode 33 - trained steps 3940 - score 142.0 - avg_score 141.5 \n","Episode 34 - trained steps 4104 - score 164.0 - avg_score 144.0 \n","Episode 35 - trained steps 4304 - score 200.0 - avg_score 151.5 \n","Episode 36 - trained steps 4504 - score 200.0 - avg_score 158.7 \n","Episode 37 - trained steps 4633 - score 129.0 - avg_score 157.6 \n","Episode 38 - trained steps 4817 - score 184.0 - avg_score 156.0 \n","Episode 39 - trained steps 5017 - score 200.0 - avg_score 170.0 \n","Episode 40 - trained steps 5217 - score 200.0 - avg_score 170.0 \n","Episode 41 - trained steps 5417 - score 200.0 - avg_score 174.5 \n","Episode 42 - trained steps 5617 - score 200.0 - avg_score 181.9 \n","Episode 43 - trained steps 5817 - score 200.0 - avg_score 187.7 \n","Episode 44 - trained steps 6017 - score 200.0 - avg_score 191.3 \n","Episode 45 - trained steps 6217 - score 200.0 - avg_score 191.3 \n","Episode 46 - trained steps 6417 - score 200.0 - avg_score 191.3 \n","Episode 47 - trained steps 6617 - score 200.0 - avg_score 198.4 \n","Episode 48 - trained steps 6817 - score 200.0 - avg_score 200.0 \n","Episode 49 - trained steps 7017 - score 200.0 - avg_score 200.0 \n","Episode 50 - trained steps 7217 - score 200.0 - avg_score 200.0 \n","Episode 51 - trained steps 7417 - score 200.0 - avg_score 200.0 \n","Episode 52 - trained steps 7617 - score 200.0 - avg_score 200.0 \n","Episode 53 - trained steps 7817 - score 200.0 - avg_score 200.0 \n","Episode 54 - trained steps 8017 - score 200.0 - avg_score 200.0 \n","Episode 55 - trained steps 8217 - score 200.0 - avg_score 200.0 \n","Episode 56 - trained steps 8417 - score 200.0 - avg_score 200.0 \n","Episode 57 - trained steps 8598 - score 181.0 - avg_score 198.1 \n","Episode 58 - trained steps 8798 - score 200.0 - avg_score 198.1 \n","Episode 59 - trained steps 8936 - score 138.0 - avg_score 191.9 \n","Episode 60 - trained steps 9114 - score 178.0 - avg_score 189.7 \n","Episode 61 - trained steps 9215 - score 101.0 - avg_score 179.8 \n","Episode 62 - trained steps 9330 - score 115.0 - avg_score 171.3 \n","Episode 63 - trained steps 9469 - score 139.0 - avg_score 165.2 \n","Episode 64 - trained steps 9585 - score 116.0 - avg_score 156.8 \n","Episode 65 - trained steps 9785 - score 200.0 - avg_score 156.8 \n","Episode 66 - trained steps 9985 - score 200.0 - avg_score 156.8 \n","Episode 67 - trained steps 10182 - score 197.0 - avg_score 158.4 \n","Episode 68 - trained steps 10382 - score 200.0 - avg_score 158.4 \n","Episode 69 - trained steps 10582 - score 200.0 - avg_score 164.6 \n","Episode 70 - trained steps 10782 - score 200.0 - avg_score 166.8 \n","Episode 71 - trained steps 10982 - score 200.0 - avg_score 176.7 \n","Episode 72 - trained steps 11169 - score 187.0 - avg_score 183.9 \n","Episode 73 - trained steps 11322 - score 153.0 - avg_score 185.3 \n","Episode 74 - trained steps 11467 - score 145.0 - avg_score 188.2 \n","Episode 75 - trained steps 11568 - score 101.0 - avg_score 178.3 \n","Episode 76 - trained steps 11693 - score 125.0 - avg_score 170.8 \n","Episode 77 - trained steps 11786 - score 93.0 - avg_score 160.4 \n","Episode 78 - trained steps 11904 - score 118.0 - avg_score 152.2 \n","Episode 79 - trained steps 12003 - score 99.0 - avg_score 142.1 \n","Episode 80 - trained steps 12139 - score 136.0 - avg_score 135.7 \n","Episode 81 - trained steps 12246 - score 107.0 - avg_score 126.4 \n","Episode 82 - trained steps 12382 - score 136.0 - avg_score 121.3 \n","Episode 83 - trained steps 12517 - score 135.0 - avg_score 119.5 \n","Episode 84 - trained steps 12698 - score 181.0 - avg_score 123.1 \n","Episode 85 - trained steps 12898 - score 200.0 - avg_score 133.0 \n","Episode 86 - trained steps 13098 - score 200.0 - avg_score 140.5 \n","Episode 87 - trained steps 13298 - score 200.0 - avg_score 151.2 \n","Episode 88 - trained steps 13498 - score 200.0 - avg_score 159.4 \n","Episode 89 - trained steps 13685 - score 187.0 - avg_score 168.2 \n","Episode 90 - trained steps 13885 - score 200.0 - avg_score 174.6 \n","Episode 91 - trained steps 14085 - score 200.0 - avg_score 183.9 \n","Episode 92 - trained steps 14285 - score 200.0 - avg_score 190.3 \n","Episode 93 - trained steps 14485 - score 200.0 - avg_score 196.8 \n","Episode 94 - trained steps 14685 - score 200.0 - avg_score 198.7 \n","Episode 95 - trained steps 14815 - score 130.0 - avg_score 191.7 \n","Episode 96 - trained steps 14932 - score 117.0 - avg_score 183.4 \n","Episode 97 - trained steps 15074 - score 142.0 - avg_score 177.6 \n","Episode 98 - trained steps 15255 - score 181.0 - avg_score 175.7 \n","Episode 99 - trained steps 15455 - score 200.0 - avg_score 177.0 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6lio08_Iqg5w","executionInfo":{"status":"ok","timestamp":1623140469538,"user_tz":-420,"elapsed":893,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}},"outputId":"a7818e1e-177d-449c-97ae-cf3533db9446"},"source":["d3qn.epsilon = 0.0\n","episodes = 5\n","for episode in range(episodes):\n","    state = env.reset()\n","    score = 0\n","    while True:\n","        action = d3qn.choose_action(state)\n","        state, reward, done, info = env.step(action)\n","        #env.render()\n","        score+=reward\n","        if done:\n","            break\n","    print('Episode: {} score: {}'.format(episode, score))"],"id":"6lio08_Iqg5w","execution_count":155,"outputs":[{"output_type":"stream","text":["Episode: 0 score: 200.0\n","Episode: 1 score: 200.0\n","Episode: 2 score: 200.0\n","Episode: 3 score: 200.0\n","Episode: 4 score: 200.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c7SAiEb8f0p5","executionInfo":{"status":"ok","timestamp":1623140478588,"user_tz":-420,"elapsed":326,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}}},"source":["#train_dir = './trained_models/CartPole/DuelingDeepQ'\n","train_dir = '.'\n","d3qn.epsilon = 0.0\n","d3qn.save_model(train_dir)"],"id":"c7SAiEb8f0p5","execution_count":156,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jrl2q5tlkUUh"},"source":["## Test"],"id":"jrl2q5tlkUUh"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPpN4cwlkXwa","executionInfo":{"status":"ok","timestamp":1623140489692,"user_tz":-420,"elapsed":412,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}},"outputId":"1c1d1ee2-4c43-4cb1-be91-b11c5412e67c"},"source":["trained_d3qn = Agent(lr=0.005, gamma=0.99, n_actions=env.action_space.n, epsilon=0.0, batch_size=64, input_dims=env.observation_space.shape)\n","trained_d3qn.load_model(train_dir, learned_steps=20637)"],"id":"VPpN4cwlkXwa","execution_count":157,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Unresolved object in checkpoint: (root).V.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).V.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).V.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).V.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).V.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).V.bias\n","WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3oDodUSoklqc","executionInfo":{"status":"ok","timestamp":1623140495209,"user_tz":-420,"elapsed":867,"user":{"displayName":"Dat Nguyen Cong","photoUrl":"","userId":"00650321240159610761"}},"outputId":"377682f3-ab42-4aee-f864-55e9fb6a3b60"},"source":["episodes = 5\n","for episode in range(episodes):\n","    state = env.reset()\n","    score = 0\n","    while True:\n","        action = trained_d3qn.choose_action(state)\n","        state, reward, done, info = env.step(action)\n","        #env.render()\n","        score+=reward\n","        if done:\n","            break\n","    print('Episode: {} score: {}'.format(episode, score))"],"id":"3oDodUSoklqc","execution_count":158,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer dense_352 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n","Episode: 0 score: 200.0\n","Episode: 1 score: 200.0\n","Episode: 2 score: 200.0\n","Episode: 3 score: 200.0\n","Episode: 4 score: 200.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZOMnvZX8j_28"},"source":["# New Section"],"id":"ZOMnvZX8j_28"},{"cell_type":"markdown","metadata":{"id":"KZ46qyDskAQh"},"source":["# New Section"],"id":"KZ46qyDskAQh"}]}